# Call for Abstracts

The CFDE Steering Committee invites you to submit an abstract of your scientific research or technical work to be considered for a poster or oral presentation during the CFDE meeting, April 4-5, 2023 at the Bethesda Hotel.

Abstracts can be of a scientific or technical nature. Abstracts should provide clear data and validated conclusions. They should consist of an introduction that presents the background and rationale of the study or work, a brief description of the methods employed, a summary of the results, and a statement of the conclusions, best practices, or lessons learned.

Because this meeting is a confidential, internal "study" meeting, abstracts submitted to other national/international conferences are eligible and welcomed for co-submission. 

Poster boards are 6 ft wide x 4 ft high. Posters can be in portrait or landscape mode but should not excede the poster stand dimensions. Space for posters and time for presentations will be limited. If the organizers of the CFDE meeting receive more abstracts than there is available space, priority will be given to those abstracts that were submitted in a timely fashion and that provide all the information that is requested. We will also be selecting a small subset of poster presenters to give brief lightning-style talks at the start of the poster session (2 min or less).

The poster reception taking place in the afternoon of Tuesday, April 4th. 

The CFDE will confirm poster and oral presentation assignments with the presenting authors by **March 28, 2023**.

## [Abstract Submission Form](https://docs.google.com/forms/d/e/1FAIpQLSdyKekDI4tnoDdj9XWGwab6N83DTtlqaNXAP3T5CvjOO7g9Kw/viewform)

Please fill out [this form](https://docs.google.com/forms/d/e/1FAIpQLSdyKekDI4tnoDdj9XWGwab6N83DTtlqaNXAP3T5CvjOO7g9Kw/viewform) to submit your abtract.

## Lightning Talks

### CAVATICA: an interoperable cloud workspace for the CFDE community
**Surya Saha** 
Introduction:  One of the aims of KFDRC CFDE is to expand end-to-end interoperability between the CFDE Portal and Cloud workspaces, enabling scientifically significant analyses of cross- Data Coordination Centers (DCC) multi-modal datasets. A key challenge in data discovery is collecting datasets spanning multiple DCCs on a cloud compute platform in a way that is both easy to use and meaningful.   Methods:  To meet this challenge, we implemented and demonstrated a GA4GH Data Repository Service (DRS) manifest-based import for CFDE DCCs on our CAVATICA platform at the Council of Councils meeting in September 2022. This DRS manifest-based import connects the associated metadata and provides a harmonized computational framework that allows a user to create a cross-Common Fund dataset cohort which, in turn, has the potential to accelerate platform-based discovery and clinical translation.   The combination of interoperable metadata and harmonized computational framework sets the stage for cross-DCC analyses and tool development. To import data into CAVATICA, a researcher can create a manifest from the CFDE portal then import it into a collaborative CAVATICA workspace. The manifest is created using NCPI file manifest standards. The user’s dbGaP access authorizations are checked by CAVATICA each time it’s accessed, and the data becomes available only if the user has proper authorization.   Conclusion:  Interoperability between the CFDE portal and the CAVATICA platform has enabled the CFDE researcher to utilize a variety of interactive tools like Jupyter notebooks and RStudio followed by execution at scale on a multi-cloud and secure environment using CWL, Nextflow or WDL workflow languages. In addition to bringing in data from DCCs, CAVATICA can host controlled data generated by CFDE researchers and provide yet another resource for the CFDE community. The CAVATICA platform is an established bioinformatics workbench that can be a cloud workspace for the entire CFDE community in the future with round the clock user support and hands-on training events.                   

### Computational Screen for Prioritizing Immunotherapeutic Targets for Endometrial Carcinoma with TargetRanger
**Vivian Utti**
Endometrial cancer (EC) is the most common malignant gynecological cancer in the United States and Canada accounting for ~7% of new cancer cases in women. The current treatment options for EC are limited to platinum-based chemotherapies which induce significant cytotoxicity. Accordingly, there are currently no molecular targeted therapies for EC.  In this study, we conducted computational analysis, using the CFDE tool TargetRanger, to identify membrane protein targets for EC. We first obtained transcriptomics data from the TCGA EC project which were downloaded from the NCI Genomic Data Commons (GDC) database. Next, we performed dimensionality reduction and automatically identified EC subtypes with the K-means clustering algorithm. For each cluster of tumors, or subtype, we identified genes that are highly expressed in the tumor cluster and lowly expressed across all human normal tissues and cell types. The data about gene expression in normal tissues and cell types was assembled from GTEx, ARCHS4 and Tabula Sapiens. We specifically seek candidates that are cell surface membrane proteins because these can be targeted with antibody-drug conjugates (ADCs) for cell removal. After identifying such targets computationally, we plan to develop ADCs for these novel targets and test their efficacy in-vitro and in xenograft mouse models of EC.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             

### Integrative analysis of multi-omics data to identify and characterize long noncoding RNA-derived fusions in pediatric cancer
**Chan Zhou**
Fusion RNAs and their encoded proteins are widely found in various cancers and have been used as biomarkers and therapeutic targets for multiple cancers. However, long noncoding RNAs (lncRNAs) derived fusion RNAs (lnc-fusions) have been largely ignored, especially in pediatric cancer, although lncRNAs largely outnumber mRNAs and play critical roles in various cancers. In this study, we identify and characterize lnc-fusions in pediatric cancer using multi-omics data from the Common Fund Datasets (Gabriella Miller Kids First, GTEx, and 4DNucleome) and external databases (GEO and TCGA). For this purpose, we developed a new pipeline that integrates our novel-lncRNA-detection method (named Flnc), three fusion detection methods, and other computational approaches. We are applying our new pipeline to detect and characterize lnc-fusions on neuroblastoma and myeloid malignancies since these pediatric cancers have large cohorts of sequencing data in the Gabriella Miller Kids First Dataset. As initial results of this study, we found hundreds of novel lnc-fusions occurring in at least one of 180 neuroblastoma patients. Among these lnc-fusions, three lnc-fusions are key lnc-fusions in neuroblastoma, which presents in at least 100 of 180 patients. One of the three lnc-fusions is the fusion transcript of a lncRNA and a tumor repressor. For this ongoing project, we are working to examine if the key lnc-fusions are related to the survival and disease severity of neuroblastoma and determine the fusion mechanisms of these lnc-fusions. We will experimentally validate the key lnc-fusions in neuroblastoma through collaboration. We will further extend this study to other pediatric cancer. Our study will complete the fusion landscape in pediatric cancers, illuminate fusion mechanisms, and enrich lncRNA functions in pediatric tumorigenesis. The critical novel lnc-fusions may serve putative diagnostic and prognostic biomarkers for pediatric cancer and may guide the development of new targeted therapy for pediatric cancer patients.

### MetGENE: Gene-centric Metabolomics Information Retrieval Tool
**Sumana Srinivasan<**
Background: Biomedical research often involves contextual integration of multi-modal and multi-omic data in search of mechanisms for improved diagnosis, treatment and monitoring. Researchers need to access information from diverse sources comprising data in various and sometimes incomplete formats. The downstream processing of the data, to decipher mechanisms by reconstructing networks and developing quantitative models, warrants considerable effort.  Methods: It is designed as a web-based application using PHP and JavaScript as the front-end and at the back-end, R scripts with wrapper functions to retrieve information from various data repositories such as KEGG  (for reaction and metabolite/compound IDs) and Metabolomics Workbench (for metabolite RefMet names and study IDs).  MetGENE maintains session variables for species ID and organism name; ENTREZ gene ID and gene symbol; anatomy, disease and phenotype terms, and the previous values of these terms to enable server-side caching of pages and thus avoid unnecessary and time-consuming fetching of data across the network. For programmatic ease of access, we provide REST APIs that convert each table of the information displayed into its corresponding JSON format. The REST APIs are developed using Smart/OpenAPI format API . Results: MetGENE is a knowledge-based, gene-centric data aggregator that hierarchically retrieves information about the gene(s), their related pathway(s), reaction(s),metabolite(s), and metabolomic studies from standard data repositories under one dashboard to enable ease of access through centralization of relevant information. Further, the information can be contextualized by filtering along species, anatomy (tissue) and condition (disease or phenotype).  MetGENE is an open source tool and is available at: https://bdcw.org/MetGENE/index.php.                                                                                                                                                                                                                                                             

### Systematic Personalized Cell Surface Target and Drug Identification from KidsFirst RNA-seq Profiling of Tumors
**Giacomo Marino**
The KidsFirst NIH Common Fund program has characterized the molecular signatures of thousands of diverse pediatric brain tumors. These patients have limited treatment options with poor survival rates. One of the most promising emerging approaches to treat these patients is by targeting tumors with immunotherapies such as antibody-drug conjugates (ADCs) and CAR T-cell therapies. However, the initial challenge with developing such immunotherapies is identifying targets and drugs that may effectively eliminate cancer cells while leaving healthy cells and tissues unharmed. By combining data collected by other Common Fund (CF) programs together with the transcriptomics and proteomics data collected by KidsFirst, we can identify and prioritize targets and drugs that may be applied as therapies tailored to the individual patient. To achieve this, we compared KidsFirst pediatric cancer expression data to normal tissues and cell types expression data from GTEx, Tabula Sapiens, ARCHS4, and HuBMAP to identify genes that are significantly upregulated in the tumors, lowly expressed across normal tissues and cell types, and encode cell surface proteins. At the same time, we also prioritized drugs and pre-clinical small-molecule compounds using the most recent LINCS L1000 data for the same patients. The workflows to perform such analyses are made available via the CFDE-supported bioinformatics web-based applications: TargetRanger and SigCom LINCS. TargetRanger is available from https://targetranger.maayanlab.cloud/; and SigCom LINCS is available from https://maayanlab.cloud/sigcom-lincs/. These bioinformatics workflows and web apps enabled the recovery of previously identified targets and small molecules, as well as several novel targets and drugs for further experimental exploration. Overall, the entire framework produces hypotheses for individualized treatments for pediatric cancer patients by integrating data from the CF programs KidFirst, LINCS, HuBMAP and GTEx.                                                                

### The CFDE Playbook Partnership Workflow Builder: An Integrative Platform to Facilitate Knowledge Discovery across Common Fund Programs’ Tools and Datasets
**Daniel Clarke**
The NIH Common Fund (CF) aims to produce large-scale datasets that may serve as resources for the research community for hypothesis generation facilitating diverse use cases. The CF supported data coordination centers (DCCs) are tasked with developing infrastructure to support the findability, accessibility, interoperability, and findability (FAIR) of the data produced by each (CF) program, with the aim of maximally extracting useful knowledge from these data. The independent DCCs repositories are suitable for specific domains of knowledge, but complex queries that span across data and tools from multiple CF programs are currently not easily possible. By taking advantage of existing FAIR APIs that serve knowledge across DCCs, many types of complex queries and workflows can be created by connecting these APIs into one framework. The Common Fund Data Ecosystem (CFDE) Playbook Partnership Workflow Builder (PPWB) is a web-based platform that facilitates knowledge resolution by enabling users to traverse an ever-growing network of input datasets, semantically annotated API endpoints, and data visualization tools contributed by the ecosystem. Via a user-friendly web-based user interface, workflows can be constructed from these building-blocks without technical expertise. The output of each step of the workflows are provided in reports containing textual descriptions, interactive figures, and downloadable tables. To demonstrate the ability of the CFDE-PPWB to generate meaningful hypotheses that draw knowledge from across multiple DCCs, we present several use cases. For example, one of these use cases sieves novel targets for an individual KidsFirst patient using data from the GTEx, LINCS, Metabolomics, GlyGen, and ExRNA DCCs. All the workflows that are created with the CFDE-PPWB platform can be reviewed, published, and repurposed to tackle similar use cases using different inputs.                                                                                                     

## Posters

### Building a Robust and Reliable API With Pydantic, FastAPI, and Schemathesis
**Joseph Okonda**
As biological projects continue to produce large amounts of data, it is increasingly important to make this data easily accessible for researchers to utilize. One way to achieve this is through the use of well-designed, robust APIs. Existing data-access APIs such as the RNAGet API or the DRS API, are designed with a single type of data in mind. Therefore, to provide access to complex, multimodal data, one often has to implement a bespoke API. Designing, developing, and maintaining such APIs, however, can be challenging, particularly in resource-constrained environments —  with small teams and limited computational resources. We describe  how we redesigned the GTEx API. In particular, we outline the design choices we made to improve the API's reliability and accessibility,  simplify the codebase in order to make it easier to  maintain, and improve the API documentation to make it more Interoperable and Reusable. Additionally, we'll discuss how we utilized free and open source software tools, key among them FasAPI, Pydantic, and Schemathesis,  to model, implement, document, and extensively test our new API.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              

### CFDE-GSKG: Gene Set Integration across Common Fund Datasets
**John Erol Evangelista**
NIH Common Fund (CF) programs have accelerated transformative science by producing massive omics datasets for the research community. A challenge remains in making these data more Findable, Accessible, Interoperable, and Reusable (FAIR), and integrating these datasets to aid researchers in querying data across CF programs. To address this challenge here we introduce the Common Fund Data Ecosystem (CFDE) Gene Set Knowledge Graph (GSKG), a knowledge graph database and web-based application that serves gene sets extracted from CF programs datasets to provide an integrated enrichment analysis tool. So far, nine gene set libraries were created by processing datasets from LINCS, GTEx, Metabolomics, IDG, GlyGen, IMPC, and HuBMAP. To use CFDE-GSKG, gene sets obtained from omics experiments by individual investigators can be submitted to the tool using a simple input form. The resulting subgraph produced by CFDE-GSKG illuminates connections between the input gene set and various CF gene sets that overlap with the queried gene set. Such analysis enables users to explore Common Fund datasets for further hypothesis generation for their research. Besides enrichment analysis, CFDE-GSKG can be used to query the consolidated CF knowledge by searching for cross-CF-dataset-associations for a gene or a biological term, for example, a drug, a metabolite, a tissue, a cell type, or a phenotype. This feature of CFDE-GSKG discovers connections between entities that may not be obvious. Future releases of CFDE-GSKG would extend the database to serve data from more Common Fund programs, as well as add predicted associations with graph-based machine learning methods, opening the doors to a broader cross Common Fund knowledge discovery resource. The CFDE-GSKG prototype is freely available at: https://cfde-gskg.dev.maayanlab.cloud/.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              

### Deep learning enabled multi-organ segmentation of mouse embryos
**Sara Rolfe**
Introduction: The IMPC has generated a repository of 3D imaging data from mouse embryos, providing a rich resource for investigating phenotype/genotype interactions. While the data is freely available, the resources required to analyze these images can create a significant hurdle for research. Manual segmentation of multiple structures is simply not feasible for many big data applications. While atlas-based methods for segmentation (ABM) can improve efficiency, these methods are computationally intensive and require specialized, high-cost compute clusters. In this work we present Mouse Embryo Multi-Organ Segmentation (MEMOS), a new deep learning-powered tool for segmentation of fetal mice, with the goal of supporting open-access analysis of imaging data collected as part of the IMPC’s KOMP2 project.  Methods: 50 labels from an E15.5 atlas were transferred to 91 baseline scans from the KOMP2 dataset to produce a “ground truth” dataset. Our segmentation model utilizes a UNETR architecture.  Results: The average Dice coefficient between the MEMOS-generated and state-of-the-art atlas-based segmentations for the test data set is 0.91. Using a high-performance server, 50 structures are estimated within two minutes. Sensitivity is quantified for the Cbx4 knockout strain, with previously reported volume reductions in the adrenals and thymic rudiments. The MEMOS-generated segmentations show a statistically significant volume decrease for the right adrenal (p=0.001), left thymic rudiment (p<0.001) and right thymic rudiment (p<0.001) in the Cbx4 knockout group. The volume decrease in the left adrenals is not statistically significant (p = .985).  Conclusions: We present MEMOS, an accessible, open-source, deep learning-enabled tool to segment microCT scans of embryonic mice (https://github.com/SlicerMorph/SlicerMEMOS). The accuracy of MEMOS-generated segmentations is comparable to gold-standard methods requiring multiple orders of magnitude more computation time and resources. MEMOS is provided as a general tool to be used in a semi-supervised pipeline for fast and highly accurate segmentations or fine-tuned for customized applications.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             

### Empowering biological discovery by making the gene regulatory information and knowledge FAIR across the CFDE
**Keyang Yu**
Introduction: Evaluating genes and their associated regulatory elements and variants in a tissue, cell, developmental, and biological context plays a critical role in understanding human disease.  However, data siloization and lack of FAIRness prevent effective deployment to enable discovery. To address this opportunity, we sought to develop data model and infrastructure for aggregating and sharing information and knowledge about gene regulation from GTEx, ENTEx, HubMAP, Roadmap Epigenome, Kids First, and ERCC projects. We piloted this approach by focusing on the regulatory relationship between genes, regulatory elements, and variants. While the infrastructure enables discovery broadly and is not tailored to any specific disease, to validate it by focusing on the interpretation of whole genome sequencing (WGS) data to discover novel Congenital Heart Disease (CHD)-causing gene regulatory variants.  Methods: To accommodate the full diversity of gene regulatory information, we developed a graph-based database CFDE Linked Data Hub (LDH). CFDE LDH provides information excerpts  and connects regulatory information from ENCODE, ENTEx, GTEx, Kids First, and Roadmap projects.  CFDE LDH currently contains about 8,000 genes and over 450M documents linking the genes with information about tissue-specific activity of ~400K regulatory elements on their tissue-specific expression, splicing quantitative trait loci, and epigenomic signatures. This information is integrated with the information about gene regulatory effects of >100M variants. To facilitate high-bandwidth data flows into and out of the LDH, we employed the Pulsar messaging system and the http2Pulsar API endpoints. To utilize this information for burden testing in Congenital Heart Disease (CHD) and other WGS studies, we also implemented a method that prioritizes candidate disease genes using Connect-the-Dots (CTD), a graph-based algorithm that enables the discovery of highly connected gene nodes within disease specific network including KEGG, STRING, and user generated datasets. Using the linked regulatory information and CTD, we identify candidate disease-causing variants in CHD Kids First cohorts.  Conclusion: By integrating disparate data resources and deploying a workflow that leverages Kids First Data Resource Center (KFDRC) and CAVATICA, we enable researchers to 1) prioritize functional noncoding variants through collapsing burden tests, 2) identify highly connected sets of genes from large gene sets, and 3) identify novel candidate disease-causing variants. While our results are still preliminary, they do suggest that aggregating multiple data sources significantly increases the power to discover disease-associated variants and that the data linking infrastructure scales to accommodate anticipated information from both CFDE and non-CFDE projects.

### Exploring the Crosstalk between Disease Pathways in Kidneys from Patients with HIV and Diabetic Nephropathy with CFDE Tools
**Zhuorui (Sherry) Xie**
The prolonged exposure to combination antiretroviral therapies (cART) for patients living with HIV promotes the development of diabetes. This increases the prevalence of diabetic kidney disease (DKD) in this patient population. However, the molecular mechanisms of DKD in patients living with HIV are mostly unknown. To address this knowledge gap, RNA-seq profiles from kidney biopsies were obtained from healthy controls, patients with diabetic nephropathy (DN) alone, and patients with DN who are also HIV positive. To analyze this dataset, we utilized the CFDE Playbook Partnership Workflow Builder (PPWB), an interactive web-based visual interface that facilitates workflow construction by connecting microservices provided by various Common Fund (CF) Data Coordination Centers (DCCs). In addition, gene sets extracted from the processed RNA-seq data were analyzed with the CFDE Gene Set Knowledge Graph (CFDE-GSKG) prototype app. Analysis and visualization using the CFDE-PPWB and CFDE-GSKG tools uniquely provide novel insights about the molecular differences in the kidney of healthy controls, DKD-HIV-, and DKD-HIV+ patients. Selected hypotheses generated from this analysis are expected to be tested in cell-based assays and animal models of the disease.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          

### Extracellular RNA (exRNA) component of the CFDE knowledge graph enables discovery of combined exRNA-RBP disease biomarkers
**Emmanuel Esquivel**
Introduction: The combination of RNA binding proteins (RBPs) and their exRNA cargo is a potential source of biomarkers of disease that have potentially superior accuracy to traditional biomarkers because they combine the specificity of proteins (RBPs) and their exRNA cargo. To facilitate search for biomarkers for specific pathological processes involving specific tissues and cell types, ERCC has developed a map of extracellular RNA (exRNA) cargo of 150 RBPs, covering 12,065 human protein coding genes. To facilitate the biomarker discovery propcess, we integrated this knowledge into the CFDE knowledge graph.  Methods: Using prior analysis on 150 RBPs over exRNA Atlas samples, we assert relationships between RBPs and four different types of Knowledge Graph nodes. RBPs are directly associated with a biofluid if their footprint was detected in exRNA-seq data from that specific biofluid. Each RBP is associated with two sets of nodes representing genomic loci. The first set consists of loci enriched in ENCODE eCLIP-seq experiments, while the second set consists of trimmed ENCODE loci that do not overlap across RBPs. Finally, RBPs are related to genes through overlap of genic regions with loci coordinate ranges.  Results: We assert that 115 RBPs carry exRNA in at least one of five biofluids examined (plasma, serum, saliva, cerebrospinal fluid, and urine). We represent ENCODE eCLIP loci for 150 RBPs as 690,919 nodes with a median of 2,553 per RBP. Trimmed loci are represented as 462,470 nodes with a median of 1,107 per RBP. Among the 115 RBPs, ENCODE loci overlap a total of 15,601 genes with a median of 1,347 across RBPs. Trimmed RBP loci overlap a total of 15,449 genes with a median of 821 across RBPs.   Conclusion: Fragments of over 15,000 gene transcripts are carried into human biofluids by only 150 RBPs examined (<10% total human RBPs), suggesting a powerful source of biomarkers for human diseases that correlate with overexpression of specific genes, including cancer, neurodegenerative and heart conditions. Queries of the Knowledge Graph will help identify the specific exRNA fragments and RBPs that may serve as biomarkers in particular biofluids for pathological processes involving overexpression of specific gene sets.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               


### GlyGen & CFDE: Integrating glycoscience data into the CFDE data ecosystem
**Jeet Vora, Rene Ranzinger**
Advancing our understanding of the roles that glycosylation plays in development and disease is frequently hindered by the diversity of the data that must be integrated to gain insight into these complex phenomena. GlyGen, initially funded by Glycoscience Common Fund with the goal of democratizing glycoscience research, has developed and implemented a data repository and knowledgebase that integrates diverse types of information, including glycan array data, glycan structures, glycan biosynthesis enzymes, glycoproteins, genomic and proteomic knowledge. To facilitate easy access to this information, an intuitive, web-based interface (<https://www.glygen.org>) has been developed to visually represent the data. In addition to the browser-based interface GlyGen also offers a data portal with integrated datasets, RESTful webservice-based APIs and a SPARQL endpoint, that allow programmatic access to integrated datasets.      GlyGen joined the CFDE effort in 2021 and since then has been part of various working group committees and partnerships. GlyGen has been submitting an increasing number of datasets to the CFDE data portal using the C2M2 data model and CFDE submission tools. These datasets include information about glycans, proteins and glycoproteins from diverse species as well as glycan array data. This data can be searched and discovered on the CFDE portal and is cross-referenced with datasets from other DCCs providing information for the same glycans, proteins and glycoproteins.  As part of the CFDE content generation initiative, GlyGen provides additional information and visual elements for glycan structures that have been used to enrich the CFDE portal webpages to help users when exploring and searching datasets.  These information packages are cross-referenced with GlyGen data entries allowing users to access GlyGen from the CFDE portal and learn more about the corresponding molecules on the GlyGen portal webpages. GlyGen is actively involved in the data distillery and the workflow playbook partnership projects, where GlyGen’s goal is to connect gene, protein and glycan information with other data types and omics data from resources such as LINCS, 4D Nucleosome, IDG, Metabolomics and Kids First. By integrating and connecting GlyGen’s glycoscience data, researchers will be able to study a detailed map of connections between glycans and genes and proteins and other -omics data, such as genomics, proteomics, metabolomics, lipidomics, and epigenomics and facilitate the discovery of new knowledge and advancement of our understanding.    GlyGen’s collaboration with CFDE DCCs and partnerships aims to facilitate exchange of knowledge and data, create and share tools and other digital resources, and provide end-user support/training, thereby supporting the mission and goals of CFDE.                                            

### IDG: diverse research, reusable datasets and analytics tools for illuminating the druggable genome and translational biomedical science
**Jeremy Yang**
The Illuminating the Druggable Genome (IDG) program is designed to improve our understanding of understudied proteins from the three most commonly drug-targeted protein families: (1) G-protein coupled receptors, (2) ion channels, and (3) protein kinases. The NIH Common Fund launched IDG in 2014. IDG seeks to catalyze research in areas of biology that are currently understudied but that have high potential to improve clinical therapeutics and human health. A key resource from this program, Pharos, aggregates protein information from several sources, so that researchers everywhere can easily access it, catalyzing their own research and helping them find new proteins of interest. IDG also supports researchers to develop technologies and resources to enable the study of understudied druggable proteins. IDG is comprised of a Knowledge Management Center (KMC), three Data and Resource Generation Centers (DRGCs), a Resource Dissemination and Outreach Center (RDOC), and awards for the development of Cutting Edge Informatics Tools (CEITs). With 2023 the final year of funding, IDG has increased focus on sustainability and FAIRness of IDG resources, well aligned with the mission of CFDE. In this poster we review, summarize, and illustrate IDG resources with emphasis on FAIRness and scientific impact, highlighting how IDG can bring unique and additional value to CFDE through integrative data science combining heterogeneous datasets across CF programs and beyond.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     

### Identification of best-matching cancer cell lines for tissue types using a consensus-based approach
**Elizabeth Chun**
The NIH Common Fund projects generated much omics data to better understand human health and disease, using model systems such as cell lines (e.g., 4DN) and primary tissues (e.g., GTEx). To integrate data generated from these requires cell-line-to-tissue matching. Furthermore, the extent to which cell lines faithfully represent respective tissues has been under scrutiny. Recent studies analyzed gene expression and other molecular profiles to identify cell lines that match representative tissue/cancer types. Using a consensus-based approach, we assessed the level of consistency in tissue predictions from published studies to identify possible improvements.   \tWe analyzed 340 cancer cell lines that were predicted for tissue/cancer types in four published studies, which applied decision-tree- or regression-based classification, or statistical adjustment followed by data alignment or correlation, primarily using gene expression data. Our analysis established a set of robust cell lines for their respective tissue/cancer types while revealing others, including widely used HCT116 and PC3 cell lines, that were repeatedly predicted to match to tissue types different from their original annotation. Our consensus-based analysis also showed that the degree of prediction consistency depended on tissue/cancer types, e.g., skin or bladder cancer cell lines robustly matched to the annotated tissue type, while cell lines derived from pancreatic adenomas, brain cancer, or cancers of squamous cell types did not. To search and visualize the prediction results described in this study, we developed an online interactive tool, PIPPIN.  \tThis study provides catalogues of cell lines that robustly recapitulate their respective tissue/cancer types and highlights the cell lines that prompt caution in their use, thus urging further verification based on molecular characteristics in the absence of absolute knowledge.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     

### Integration of BioCompute Objects Into the CFDE Playbook Partnership
**Jonathon Keeney**
BioCompute is a standardized way of communicating computational workflow information. The standard is built on existing data standards and ontologies for accurately identifying versioned resources, tracking data provenance, user attribution, and other features. The standard is extensible, and can be used in combination with other standards, such as workflow languages for portability of execution.  The standard is officially called IEEE 2791-2020, and an instance of a workflow written according to the standard is called a BioCompute Object (BCO). The project was conceived during the planning of NCBI's BioProject, and steadily gained momentum, with more than 400 individuals having weighed in on the project by the time it was standardized. Shortly after standardization, the project was adopted by three Centers at the US FDA. The project aims to standardize information exchange and communication between organizations, researchers, and industries.  The CFDE Workflow Playbook Partnership seeks to develop workflows that draw knowledge from across Common Fund Data Coordination Centers (DCCs) using a common system, to enable researchers to synthesize information stored in multiple DCCs to accumulate evidence for specific hypotheses. The system is built as an API-driven constellation of microservices that all DCCs connect to. Here, we present BioCompute as part of the foundation on which the API-driven system is built. Users may come to the site with specific data in hand (e.g. a \hit list\ of genes) in search of connections to other data (e.g. glycosylation). The BCO record of their research will record a manifest of unique identifiers for all inputs (e.g. RefSeq accessions) and versioned resources, as well as all parameters for the search. The BCO documents the workflow, and can be saved, searched, shared or reproduced. In this way, BioCompute acts like a lingua franca to harmonize workflow data. An example GlyGen workflow is shown.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             

### Integration of the new GTEx API with Portal Front-end, and its impact on FAIRNESS
**Qiuyue Liu**
FAIRness (Findable, Accessible, Interoperable, Reusable) has been an important focus for data objects in CFDE. It helps facilitate advanced and ongoing scientific discoveries after the initial creation of data objects. The Genotype-Tissue Expression (GTEx) project has created a myriad of diverse scientific data and is aimed at providing the scientific community with data and resources to study human gene expression and regulation. The GTEx portal serves as the main entry point to the project by providing researchers with GTEx’s public access data as well as visualizations of the gene expression data.  FAIRness evaluations of the GTEx portal, conducted as part of our CFDE engagement, have shown that the portal has room for improvement, specifically in the area of APIs. The portal has always used an in-house-developed and maintained API to support visualizations and data retrieval. To improve our FAIRness metrics in this area, and in order to allow for more accessibility of GTEx data, the GTEx team recently launched a new version of our in-house API. Here we describe the process of integration and implementation of the new API with the portal’s front end, specifically visualizations. During the integration process, we faced difficulties that include third party application compatibility and different data formats, and had to make trade-offs to balance consistency and performance.  We present these difficulties and tradeoffs as considerations for other CFDE  API implementation and integration efforts.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          

### MetENP/MetENPWeb: An R Package and Web Application for Metabolomics Enrichment and Pathway Analysis
**Mano R. Maurya**
Metabolomics deals with the measurement and analysis of metabolites in biochemical and biological samples. Metabolomics is one of the most rapidly growing fields of multi-omics. Metabolomics analysis can be utilized to correlate metabolites to the phenotypic characteristics. A recent surge in the generation of large-scale metabolomics datasets warrants the need for integrated tools that not only capture important metabolites but also perform predictive in silico analysis of downstream pathways and gene function associated with the altered metabolites. Here, we introduce MetENP, an R package, and a user-friendly web interface deployed at Metabolomics Workbench (MetENPweb) that extends the metabolomics enrichment analysis to include species-specific pathway analysis, pathway enrichment score, and the related information on the associated metabolic reactions, such as gene-enzyme information, for the significantly altered metabolites. MetENP provides a highly customizable workflow through a variety of user-specified options and includes support for all species with available KEGG pathways. MetENP is available as an R package and Jupyter notebook (https://github.com/metabolomicsworkbench/MetENP) and a Web resource <https://www.metabolomicsworkbench.org/data/analyze.php>. A tutorial is available at <https://www.metabolomicsworkbench.org/data/MW-MetENP-demo.pdf>.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       

### OmicsPipelines: A cloud-native desktop and web application for launching Cromwell-based proteomics and genomics pipelines
**Mihir Samdarshi**
Here we present an open-source web and desktop application for launching analysis pipelines of  large-scale proteomics and genomics datasets in cloud environments. We illustrate that OmicsPipeline can easily be set up and run with very limited knowledge of cloud computing. These curated genomics and proteomics pipelines are composed of a number of discrete steps that apply some combination of algorithms, processing, and software tools to provide reproducible analysis outputs.   The OmicsPipeline application consists of two modules: an installer and a dashboard. Both are presented with an easy to use graphical user interface. By providing a convenient user interface and sensible default configurations, we attempt to simplify complicated modern data processing pipelines, and the complex behemoth that is modern-day cloud platforms. The “installer” is a cross-platform desktop application that automatically creates all the necessary infrastructure components in a chosen cloud provider (Amazon Web Services or Google Cloud Platform) required to orchestrate Cromwell jobs. The dashboard allows the user to create, run, and monitor Proteomics and GET (Genomics, Epigenomics, and Transcriptomics) workflows.   Once a specific workflow is selected, the user is presented with a simple form with (customizable) default values to customize the files and parameters required to run the pipeline. Multiple users can access the dashboard, facilitating collaborative work. It is also possible to install and access multiple dashboard instances. The framework provides extensive support through documentation, tutorials, and a forum to request help and enable user discussions.  In summary, we introduce OmicsPipeline, a comprehensive cloud-native application for the analysis of both high-throughput proteomics and GET experiments, enabling the adoption of cloud technologies and its key features. By having available fast, scalable, and reproducible pipelines, we can enable the easier reuse of data and support testing or adoption of newer software tools, and the analysis of small and gigantic datasets.                                                                                                                                                                                               
